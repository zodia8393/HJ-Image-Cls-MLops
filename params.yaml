seed: 42
data:
  name: cifar10
  root: ./src/data
  num_workers: 4
  persistent_workers: true

train:
  # CIFAR-10 권장 Epoch는 120~200이 일반적이나, 최적화/증강/백본으로 60에 수렴 가속
  epochs: 60
  batch_size: 512           # GPU 허용 시 512, 부족하면 256로
  lr: 0.1                   # SGD 기준 시작 학습률 (batch_size 512 기준)
  momentum: 0.9
  nesterov: true
  weight_decay: 0.0005
  warmup_epochs: 5
  scheduler: cosine
  amp: true
  grad_clip: 0.0            # CIFAR-10/SGD에서는 보통 불필요, 필요시 1.0

  # 과적합 방지용
  label_smoothing: 0.1
  mixup: 0.0                # 필요시 0.2
  cutmix: 0.0               # 필요시 0.5
  dropout: 0.0              # WideResNet 기본은 Dropout 포함 가능(아래 모델에서 설정)
  drop_path: 0.0            # WRN에서는 일반적으로 미사용

aug:
  # 속도-성능 균형: RandAug 약화 + Cutout
  randaug_m: 7
  randaug_n: 2
  cutout: 8                 # 패치 크기(픽셀), 0이면 비활성화
  random_erase_prob: 0.0    # CIFAR-10에서는 Cutout로 충분한 경우 많음

model:
  arch: wrn28_10_cifar       # WideResNet-28-10 (CIFAR 전용)
  num_classes: 10
  img_size: 32               # CIFAR-10 고정
  pretrained: false          # CIFAR-10 전용 구조는 사전학습 가중치 거의 없음
